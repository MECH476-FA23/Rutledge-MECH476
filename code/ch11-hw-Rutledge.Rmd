---
title: 'MECH481A6: Engineering Data Analysis in R'
subtitle: 'Chapter 11 Homework: Modeling' 
author: 'Ethan Rutledge'
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: pdf_document
---

```{r global-options, include=FALSE}
# set global options for figures, code, warnings, and messages
knitr::opts_chunk$set(fig.width=6, fig.height=4, fig.path="../figs/",
                      warning=FALSE, message=FALSE)
```

# Load packages

```{r load-packages, message=FALSE}
# load packages for current session
library(tidyverse) 
library(gridExtra) 
```

# Chapter 11 Homework

This homework will give you experience with OLS linear models and testing their assumptions.  

For this first problem set, we will examine issues of ***collinearity among predictor variables*** when fitting an OLS model with two variables. As you recall, assumption 3 from OLS regression requires there be no *collinearity* among predictor variables (the $X_i$'s) in a linear model.  The reason is that the model struggles to assign the correct $\beta_i$ values to each predictor when they are strongly correlated.   

## Question 1
Fit a series of three linear models on the `bodysize.csv` data frame using `lm()` with `height` as the dependent variable:  
  1. Model 1: use `waist` as the independent predictor variable:  
        - `formula = height ~ waist`   
  2. Model 2: use `mass` as the independent predictor variable:  
        - `formula = height ~ mass`  
  3. Model 3: use `mass + waist` as a linear combination of predictor variables:  
        - `formula = waist + mass`  
    
Report the coefficients for each of these models.  What happens to the sign and magnitude of the `mass` and `waist` coefficients when the two variables are included together?  Contrast that with the coefficients when they are used alone.

Evaluate assumption 3 about whether there is collinearity among these variables.  Do you trust the coefficients from model 3 after having seen the individual coefficients reported in models 1 and 2?


```{r ch11-homework-q1}
bodysize <- read_csv("../data/bodysize.csv")

model_1 <- lm(height ~ waist, data = bodysize)

model_2 <- lm(height ~ mass, data = bodysize)

model_3 <- lm(height ~ waist + mass, data = bodysize)

summary(model_1)
summary(model_2)
summary(model_3)

```

# Answer: 
## See output for coefficients of each model
## When the two variables are included together the slope is negative for waist wherein the others it was always positive also the std error associated is much larger
## No I dont trust model 3 based on the coefficients of model 1 and 2. They seem to have strong collinearity and so the model isnt set up well.


## Question 2
Create a new variable in the `bodysize` data frame using `dplyr::mutate`. Call this variable `volume` and make it equal to $waist^2*height$.  Use this new variable to predict `mass`.  

```{r ch11-homework-q2}
bodysize <- bodysize%>%
  mutate(volume = waist ^ 2 * height)
```

Does this variable explain more of the variance in `mass` from the NHANES data? How do you know? (hint: there is both *process* and *quantitative* proof here)

```{r ch11-homework-q2a}
model_4 <- lm(mass ~ volume, data = bodysize)
summary(model_4)
```

# Answer: Yes, volume explains the variance in mass much better than any of the other models. This is most clear when you look at the r^2 values as this one has the highest.

Create a scatterplot of `mass` vs. `volume` to examine the fit.  Draw a fit line using `geom_smooth()`.

```{r ch11-homework-q2b}
ggplot(bodysize,aes(x = mass, y = volume)) + geom_point(alpha = 0.5) + geom_smooth() + labs(x="volume", y = "mass", title = "mass vs. volume scatterplot with fit line") + theme_minimal()
```

## Question 3
Load the `cal_aod.csv` data file and fit a linear model with `aeronet` as the independent variable and `AMOD` as the independent variable. 
```{r ch11-homework-q3}
# load data
cal_aod <- read_csv("../data/cal_aod.csv")

model_cal_aod <- lm(aeronet ~ amod, data = cal_aod)

```

Evaluate model assumptions 4-7 from the coursebook.  Are all these assumptions valid? 

```{r ch11-homework-q3a}
#assumption 4: mean of residuals is zero
mean_residuals <- mean(model_cal_aod$residuals)
mean_residuals

```

#Answer: 
##Assumption4: The error term has a mean of zero
## Valid: close enought to call it zero

```{r ch11-homework-q3b}
#assumption 5: residuals are normally distributed
hist(model_cal_aod$residuals)

```

#Answer:
## Assumption 5: The error term is normally distributed
##

```{r ch11-homework-q3c}
#assumption 6: the error term is homoscedastic
ggplot() + geom_point(aes(x=model_cal_aod$fitted.values, y=model_cal_aod$residuals)) + labs(x="Fitted Values", y="Residuals") + theme_minimal()

```

# Answer:
## Assumption 6: The error term is homoscedastic
##Valid: the magnitude of the residuals is constant as the fitted values increase 

```{r ch11-homework-q3d}
#assumption 7: no autocorrelation among residuals
acf(model_cal_aod$residuals)

```

#Answer:
## Assumption 7: No autocorrelation among residuals
## Valid: the data stays evenly around 0.0. There is one outlier that appeared also in the previous question but I believe it is an outlier and can be disregared.